{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0. **Import external Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import calendar\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import geopandas as geopd\n",
    "import country_converter as coco\n",
    "from functools import reduce\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Optimization\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import (\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T17:43:15.105254Z",
     "iopub.status.busy": "2022-11-15T17:43:15.104207Z",
     "iopub.status.idle": "2022-11-15T17:43:15.753391Z",
     "shell.execute_reply": "2022-11-15T17:43:15.752177Z",
     "shell.execute_reply.started": "2022-11-15T17:43:15.105215Z"
    }
   },
   "outputs": [],
   "source": [
    "exam_info = pd.read_csv(\"365_database/365_exam_info.csv\")\n",
    "course_info = pd.read_csv(\"365_database/365_course_info.csv\")\n",
    "student_info = pd.read_csv(\"365_database/365_student_info.csv\")\n",
    "course_ratings = pd.read_csv(\"365_database/365_course_ratings.csv\")\n",
    "student_learning = pd.read_csv(\"365_database/365_student_learning.csv\")\n",
    "student_exams = pd.read_csv(\"365_database/365_student_exams.csv\")\n",
    "quiz_info = pd.read_csv(\"365_database/365_quiz_info.csv\")\n",
    "student_quizzes = pd.read_csv(\"365_database/365_student_quizzes.csv\")\n",
    "student_hub_questions = pd.read_csv(\"365_database/365_student_hub_questions.csv\")\n",
    "student_engagement = pd.read_csv(\"365_database/365_student_engagement.csv\")\n",
    "student_purchases = pd.read_csv(\"365_database/365_student_purchases.csv\")\n",
    "world_df = geopd.read_file(\"365_database/countries.geojson\")\n",
    "world_json = json.load(open(\"365_database/countries.geojson\", \"r\"))\n",
    "state_id_map = eval(open('365_database/state_id_map.json').read());\n",
    "\n",
    "id_count = 0\n",
    "for feature in world_json[\"features\"]:\n",
    "    feature[\"id\"] = id_count\n",
    "    id_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tables OVERVIEW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **exam_info:** 'exam_id', 'exam_category', 'exam_duration'\n",
    "- **course_info:** 'course_id', 'course_title'\n",
    "- **student_info:** 'student_id', 'student_country', 'date_registered'\n",
    "- **course_ratings:** 'course_id', 'student_id', 'course_rating', 'date_rated'\n",
    "- **student_learning:** 'student_id', 'course_id', 'minutes_watched', 'date_watched'\n",
    "- **student_exams:** 'exam_attempt_id', 'student_id', 'exam_id', 'exam_result', 'exam_completion_time', 'date_exam_completed'\n",
    "- **quiz_info:** 'quiz_id', 'question_id', 'answer_id', 'answer_correct'\n",
    "- **student_quizzes:** 'student_id', 'quiz_id', 'question_id', 'answer_id'\n",
    "- **student_hub_questions:** 'hub_question_id', 'student_id', 'date_question_asked'\n",
    "- **student_engagement:** 'engagement_id', 'student_id', 'engagement_quizzes', 'engagement_exams', 'engagement_lessons', 'date_engaged'\n",
    "- **student_purchases:** 'purchase_id', 'student_id', 'purchase_type', 'date_purchased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dates(filt_df, range_dates, date_column_name):\n",
    "    filt_df[date_column_name] = pd.to_datetime(filt_df[date_column_name])\n",
    "    mask = (filt_df[date_column_name] >= pd.to_datetime(range_dates[0])) & (\n",
    "        filt_df[date_column_name] <= pd.to_datetime(range_dates[1])\n",
    "    )\n",
    "    filtered_df = filt_df.loc[mask]\n",
    "    filtered_df.sort_values([date_column_name], ascending=True, inplace=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_df(df, time_column_name, range_dates, grouper, grouper_metric):\n",
    "    df[time_column_name] = pd.to_datetime(df[time_column_name])\n",
    "    mask = (df[time_column_name] >= pd.to_datetime(range_dates[0])) & (\n",
    "        df[time_column_name] <= pd.to_datetime(range_dates[1])\n",
    "    )\n",
    "    filtered_df = df.loc[mask]\n",
    "    df = filtered_df.sort_values([time_column_name], ascending=True)\n",
    "\n",
    "    if range_dates[0] != range_dates[1]:\n",
    "        if grouper_metric == \"sum\":\n",
    "            grouped_df = filtered_df.groupby(grouper).sum().reset_index()\n",
    "        if grouper_metric == \"mean\":\n",
    "            grouped_df = filtered_df.groupby(grouper).mean().reset_index()\n",
    "        if grouper_metric == \"count\":\n",
    "            grouped_df = filtered_df.groupby(grouper).count().reset_index()\n",
    "\n",
    "        return grouped_df\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_map(df, time_column_name, range_dates, grouper):\n",
    "    grouper_metric = \"sum\"\n",
    "    df = grouped_df(df, time_column_name, range_dates, grouper, grouper_metric)\n",
    "    df[\"id\"] = df[\"Country_Name\"].apply(lambda x: state_id_map[x])\n",
    "    fig = px.choropleth_mapbox(\n",
    "        df,\n",
    "        locations=\"id\",\n",
    "        geojson=world_json,\n",
    "        color=\"N째 of Registrations\",\n",
    "        hover_name=\"Country_Name\",\n",
    "        hover_data={'N째 of Registrations':True, 'id': False},\n",
    "        # cmap = 'salmon',\n",
    "        title=\"Alumni Registrations around the Globe\",\n",
    "        color_continuous_scale=\"thermal\",  # aggrnyl - bluered - cividis - darkmint - deep - thermal\n",
    "        mapbox_style=\"white-bg\",  # open-street-map - carto-positron - white-bg - stamen-toner\n",
    "        zoom=0.3,\n",
    "        opacity=0.9,\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1. **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Student Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = coco.CountryConverter()\n",
    "merge_1 = student_info.groupby([ 'student_country', 'date_registered']).count().reset_index()\n",
    "merge_1.columns.name = merge_1.index.name\n",
    "merge_1.index.name = None\n",
    "merge_1['student_country'] = cc.pandas_convert(series= merge_1['student_country'], to='ISO3')\n",
    "merge_1['student_country'].replace(to_replace=\"XKX\", value=\"-99\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_info_map = pd.merge(\n",
    "    merge_1,\n",
    "    world_df,\n",
    "    how=\"outer\",\n",
    "    left_on=\"student_country\",\n",
    "    right_on=\"ISO_A3\",\n",
    ").dropna(subset=[\"student_id\", \"geometry\"]).drop(columns='student_country')\n",
    "student_info_map.columns = ['Registration_Date', 'N째 of Registrations', 'Country_Name', 'Country_ISO_A3', 'geometry']\n",
    "student_info_map[\"id\"] = student_info_map[\"Country_Name\"].apply(lambda x: state_id_map[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student_info_map.to_csv('map_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_count = 0\n",
    "for feature in world_json[\"features\"]:\n",
    "    feature[\"id\"] = id_count\n",
    "    id_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_info_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dates(filt_df, range_dates, date_column_name):\n",
    "    filt_df[date_column_name] = pd.to_datetime(filt_df[date_column_name])\n",
    "    mask = (filt_df[date_column_name] >= pd.to_datetime(range_dates[0])) & (\n",
    "        filt_df[date_column_name] <= pd.to_datetime(range_dates[1])\n",
    "    )\n",
    "    filtered_df = filt_df.loc[mask]\n",
    "    filtered_df.sort_values([date_column_name], ascending=True, inplace=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_column_name = 'Registration_Date'\n",
    "range_dates = [\"2022-01-01\", \"2022-10-20\"]\n",
    "colors = \"N째 of Registrations\"\n",
    "hover_names =  \"Country_Name\"\n",
    "\n",
    "filtered_df = filter_by_dates(student_info_map, range_dates, date_column_name)\n",
    "df_map = filtered_df.groupby([\"Country_Name\", \"Country_ISO_A3\"]).sum().reset_index()\n",
    "df_map[\"id\"] = df_map[\"Country_Name\"].apply(lambda x: state_id_map[x])\n",
    "df_map\n",
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "        df_map,\n",
    "        locations=\"id\",\n",
    "        geojson=world_json,\n",
    "        color=colors,\n",
    "        hover_name=hover_names,\n",
    "        hover_data={colors:True, 'id': False},\n",
    "        title=\"Alumni Registrations around the Globe\",\n",
    "        color_continuous_scale=\"thermal\",  # aggrnyl - bluered - cividis - darkmint - deep - thermal\n",
    "        mapbox_style=\"white-bg\",  # open-street-map - carto-positron - white-bg - stamen-toner\n",
    "        zoom=0,\n",
    "        opacity=0.9,\n",
    "    )\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Courses DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def courses_popularity(range_dates):\n",
    "    course_rating_1 = filter_by_dates(course_ratings, range_dates, 'date_rated').groupby('course_id').mean().reset_index().drop(columns='student_id')\n",
    "    course_rating_2 = filter_by_dates(student_learning, range_dates, 'date_watched').groupby('course_id').sum().reset_index().drop(columns='student_id')\n",
    "    course_rating_3 = filter_by_dates(student_learning, range_dates, 'date_watched').groupby('course_id').mean().reset_index().drop(columns='student_id')\n",
    "    course_rating_4 = filter_by_dates(student_learning, range_dates, 'date_watched').groupby('course_id').count().reset_index().drop(columns='student_id')\n",
    "    df_to_merge =[\n",
    "        course_rating_1,\n",
    "        course_rating_2,\n",
    "        course_rating_3,\n",
    "        course_rating_4,\n",
    "        course_info\n",
    "    ]\n",
    "\n",
    "    course_info_names = reduce(lambda  left,right: pd.merge(left,right,on=['course_id'], how='outer'), df_to_merge).drop(columns='date_watched')\n",
    "    course_info_names.columns = ['course_id', 'avg_course_rating', 'tot_minutes_watched', 'avg_minutes_watched', 'times_watched', 'course_title']\n",
    "    course_info_names['intro_word'] =  course_info_names['course_title'].str.contains(pat ='Intro[a-z]', regex = True)\n",
    "    course_info_names['ml'] =  course_info_names['course_title'].str.contains(pat ='Mach[a-z]', regex = True)\n",
    "    course_info_names['popularity_1'] = course_info_names['times_watched'] / course_info_names['avg_course_rating']\n",
    "    course_info_names['popularity_2'] = course_info_names['times_watched'] * course_info_names['avg_course_rating']\n",
    "    course_info_names['popularity_3'] = course_info_names['popularity_2'] / course_info_names['tot_minutes_watched']\n",
    "    course_info_names['popularity_normal']=(course_info_names['popularity_1']-course_info_names['popularity_1'].min())/(course_info_names['popularity_1'].max()-course_info_names['popularity_1'].min())\n",
    "    course_info_names.sort_values(by='popularity_2', ascending=False)\n",
    "    return course_info_names.sort_values(by='popularity_2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_dates = [\"2022-01-01\", \"2022-10-20\"]\n",
    "courses_rating_df = courses_popularity(range_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dates DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dates = student_info.groupby('date_registered').count().reset_index().iloc[:,:2]\n",
    "reg_dates.columns=['date', 'students_registered']\n",
    "reg_dates['date'] = pd.to_datetime(reg_dates['date'])\n",
    "reg_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dates = course_ratings.groupby('date_rated').count().reset_index().iloc[:,:2]\n",
    "rating_dates.columns=['date', 'courses_rated']\n",
    "rating_dates['date'] = pd.to_datetime(rating_dates['date'])\n",
    "rating_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_date_a = student_learning.groupby('date_watched').sum().reset_index().drop(columns=['student_id','course_id'])\n",
    "watching_date_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_date_b = student_learning.groupby('date_watched').count().reset_index().drop(columns=['student_id','minutes_watched'])\n",
    "watching_date_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_date = pd.merge(\n",
    "    watching_date_a,\n",
    "    watching_date_b,\n",
    "    how='outer',\n",
    "    left_on = 'date_watched',\n",
    "    right_on  = 'date_watched'\n",
    ")\n",
    "watching_date.columns = ['date', 'tot_minutes_watched', 'tot_courses_watched']\n",
    "watching_date['date'] = pd.to_datetime(watching_date['date'])\n",
    "watching_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_dates_a = student_exams.groupby('date_exam_completed').mean().reset_index().drop(columns=['exam_attempt_id', 'student_id', 'exam_id'])\n",
    "exams_dates_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_dates_b = student_exams.groupby('date_exam_completed').count().reset_index().iloc[:,:2]\n",
    "exams_dates_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_dates = pd.merge(\n",
    "    exams_dates_a,\n",
    "    exams_dates_b,\n",
    "    how='outer',\n",
    "    left_on = 'date_exam_completed',\n",
    "    right_on  = 'date_exam_completed'\n",
    ")\n",
    "exams_dates.columns = ['date', 'avg_exam_results', 'avg_exam_completion_time', 'tot_exams_presented']\n",
    "exams_dates['date'] = pd.to_datetime(exams_dates['date'])\n",
    "exams_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dates = student_hub_questions.groupby('date_question_asked').count().reset_index().iloc[:,:2]\n",
    "questions_dates.columns=['date', 'questions_posted']\n",
    "questions_dates['date'] = pd.to_datetime(questions_dates['date'])\n",
    "questions_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_dates = student_engagement.groupby('date_engaged').sum().reset_index().drop(columns=['engagement_id', 'student_id'])\n",
    "engagement_dates.columns= ['date', 'engagement_quizzes', 'engagement_exams', 'engagement_lessons']\n",
    "engagement_dates['date'] = pd.to_datetime(engagement_dates['date'])\n",
    "engagement_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_dates_a = student_purchases.groupby(['date_purchased', 'purchase_type']).count().reset_index()\n",
    "purchases_dates = purchases_dates_a.pivot('date_purchased', 'purchase_type', 'purchase_id').reset_index()\n",
    "purchases_dates.columns = ['date', 'annual_purchases', 'monthly_purchases', 'quarterly_purchases']\n",
    "purchases_dates['date'] = pd.to_datetime(purchases_dates['date'])\n",
    "purchases_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_weekdays(df, cat_column):\n",
    "    days_cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    cat_type = CategoricalDtype (categories=days_cats, ordered=True)\n",
    "    df[cat_column] = df[cat_column].astype(cat_type)\n",
    "    return df\n",
    "\n",
    "\n",
    "dates_df = [\n",
    "    reg_dates,\n",
    "    rating_dates,\n",
    "    watching_date,\n",
    "    exams_dates,\n",
    "    questions_dates,\n",
    "    engagement_dates,\n",
    "    purchases_dates,\n",
    "]\n",
    "    \n",
    "dates_dataframe = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"date\"], how=\"outer\"), dates_df\n",
    ")\n",
    "dates_dataframe['weekday'] = dates_dataframe['date'].apply(lambda x: calendar.day_name[x.weekday()])\n",
    "dates_dataframe = categorize_weekdays(dates_dataframe, 'weekday')\n",
    "dates_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dates_dataframe.to_csv('dates_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_purchases = dates_dataframe.groupby('weekday')['annual_purchases', 'monthly_purchases','quarterly_purchases' ].sum().reset_index()\n",
    "weekday_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_engagement = dates_dataframe.groupby('weekday')['questions_posted', 'engagement_lessons', 'engagement_exams','engagement_quizzes' ].sum().reset_index()\n",
    "weekday_engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_registrations = dates_dataframe.groupby('weekday')['students_registered'].sum().reset_index()\n",
    "weekday_registrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_time_watched = dates_dataframe.groupby('weekday')['tot_minutes_watched', 'tot_courses_watched'].sum().reset_index()\n",
    "weekday_time_watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_exams_a = dates_dataframe.groupby('weekday')['avg_exam_results', 'avg_exam_completion_time'].mean().reset_index()\n",
    "weekday_exams_b = dates_dataframe.groupby('weekday')['tot_exams_presented'].sum().reset_index()\n",
    "weekday_exams = pd.merge(\n",
    "    weekday_exams_a,\n",
    "    weekday_exams_b,\n",
    "    how='outer',\n",
    "    on='weekday'\n",
    ")\n",
    "weekday_exams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Students DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_courses = student_learning.groupby('student_id').count().reset_index()[[\"student_id\", \"course_id\"]]\n",
    "student_exams_count = student_exams.groupby('student_id').count().reset_index().iloc[:,:2]\n",
    "student_exams_count.columns=['student_id', 'exams_attempts']\n",
    "student_exams_avg = student_exams.groupby('student_id').mean().reset_index().drop(columns=['exam_attempt_id', 'exam_id'])\n",
    "student_exams_avg.columns=['student_id', 'avg_exam_result', 'avg_exam_completion_time']\n",
    "student_quizzes_1  = student_quizzes.groupby('student_id').count().reset_index().iloc[:,:2]\n",
    "student_quizzes_1.columns=['student_id', 'quizz_quesitons_answered']\n",
    "student_hub_questions_1 = student_hub_questions.groupby('student_id').count().reset_index().iloc[:,:2]\n",
    "student_hub_questions_1.columns=['student_id', 'questions_posted']\n",
    "student_engagement_1 = student_engagement.groupby('student_id').sum().reset_index().drop(columns='engagement_id')\n",
    "student_purchases_0 = student_purchases.groupby(['student_id']).count().reset_index().iloc[:,:2]\n",
    "student_purchases_0.columns = ['student_id', 'tot_purchases' ]\n",
    "student_quizzes_1  = student_quizzes.groupby('student_id').count().reset_index().iloc[:,:2]\n",
    "student_quizzes_1.columns=['student_id', 'quizz_quesitons_answered']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_watch = student_learning.groupby('student_id').min().reset_index()[['student_id','date_watched']].rename(columns={\"date_watched\": \"first_watched\"})\n",
    "first_exam = student_exams.groupby('student_id').min().reset_index()[['student_id','date_exam_completed']].rename(columns={\"date_exam_completed\": \"first_exam\"})\n",
    "first_question = student_hub_questions.groupby('student_id').min().reset_index()[['student_id','date_question_asked']].rename(columns={\"date_question_asked\": \"first_question\"})\n",
    "first_engagement = student_engagement.groupby('student_id').min().reset_index()[['student_id','date_engaged']].rename(columns={\"date_engaged\": \"first_engagement\"})\n",
    "first_purchase = student_purchases.groupby('student_id').min().reset_index()[['student_id','date_purchased']].rename(columns={\"date_purchased\": \"first_purchase\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_dfs = [\n",
    "    student_info,\n",
    "    student_courses,\n",
    "    student_exams_count,\n",
    "    student_exams_avg,\n",
    "    student_quizzes_1,\n",
    "    student_hub_questions_1,\n",
    "    student_engagement_1,\n",
    "    student_purchases_0,\n",
    "    student_quizzes_1,\n",
    "    first_watch,\n",
    "    first_exam,\n",
    "    first_question,\n",
    "    first_engagement,\n",
    "    first_purchase,\n",
    "]\n",
    "\n",
    "student_all_info = reduce(lambda  left,right: pd.merge(left,right,on=['student_id'], how='outer'), students_dfs).drop(columns='tot_purchases')\n",
    "student_all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_all_info['date_registered'] = pd.to_datetime(student_all_info['date_registered'])\n",
    "student_all_info['first_watched'] = pd.to_datetime(student_all_info['first_watched'])\n",
    "student_all_info['first_exam'] = pd.to_datetime(student_all_info['first_exam'])\n",
    "student_all_info['first_question'] = pd.to_datetime(student_all_info['first_question'])\n",
    "student_all_info['first_engagement'] = pd.to_datetime(student_all_info['first_engagement'])\n",
    "student_all_info['first_purchase'] = pd.to_datetime(student_all_info['first_purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = student_all_info\n",
    "s_df['watched_before_purchase'] = s_df.apply(lambda x : True if x['first_watched'] < x['first_purchase'] and x['first_watched'] != pd.NaT else False, axis=1)\n",
    "s_df['exam_before_purchase'] = s_df.apply(lambda x : True if x['first_exam'] < x['first_purchase'] and x['first_exam'] != pd.NaT else False, axis=1)\n",
    "s_df['questioned_before_purchase'] = s_df.apply(lambda x : True if x['first_question'] < x['first_purchase'] and x['first_question'] != pd.NaT else False, axis=1)\n",
    "s_df['engaged_before_purchase'] = s_df.apply(lambda x : True if x['first_engagement'] < x['first_purchase'] and x['first_engagement'] != pd.NaT else False, axis=1)\n",
    "s_df['time_to_purchase'] = s_df['first_purchase'] - s_df['date_registered']\n",
    "s_df['time_to_date'] = s_df['date_registered'].max() - s_df['date_registered']\n",
    "s_df[\"first_purchase\"].fillna(0, inplace=True)\n",
    "s_df['purchased_yes_no']= s_df[\"first_purchase\"] != 0\n",
    "s_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_to_float(d):\n",
    "    try:\n",
    "        if isinstance(d, float) or isinstance(d, int):\n",
    "            return d\n",
    "    except:   \n",
    "        None     \n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    total_seconds =  (d - epoch).total_seconds()\n",
    "    # total_seconds will be in decimals (millisecond precision)\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "# d = s_df['first_purchase'].values[2]\n",
    "# datetime_to_float(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_purchase = s_df['first_purchase'].values\n",
    "purchases = []\n",
    "for n in first_purchase:\n",
    "    purchases.append(datetime_to_float(n))\n",
    "\n",
    "s_df['first_purchase'] = purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\n",
    "    'date_registered',\n",
    "    'first_watched',\n",
    "    'first_exam',\n",
    "    'first_question',\n",
    "    'first_engagement',\n",
    "    'time_to_date',\n",
    "    'time_to_purchase'\n",
    "]\n",
    "\n",
    "for c in date_columns:\n",
    "    s_df[c] =  s_df[c].values.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_purchases = s_df['purchased_yes_no'].value_counts()\n",
    "count_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2. **Feautre Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_1 = s_df.copy().drop(\n",
    "    columns=[\n",
    "        \"engaged_before_purchase\",\n",
    "        \"questioned_before_purchase\",\n",
    "        \"time_to_purchase\",\n",
    "        \"watched_before_purchase\",\n",
    "        \"first_purchase\",\n",
    "        \"exam_before_purchase\",\n",
    "        \"first_exam\"\n",
    "    ]\n",
    ")\n",
    "s_df_1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = s_df_1[['student_id', 'student_country']]\n",
    "countries_encode = pd.pivot_table(sub_df, values='student_id', columns=['student_country'], index='student_id', aggfunc=np.count_nonzero).reset_index().fillna(0)\n",
    "countries_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_hot = reduce(lambda  left,right: pd.merge(left,right,on=['student_id'], how='outer'), [s_df_1, countries_encode]).drop(columns='student_country')\n",
    "s_df_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of Label Encoder\n",
    "labelencoder = LabelEncoder()\n",
    "s_df_label = s_df_1.copy()\n",
    "s_df_label[\"student_country\"] = labelencoder.fit_transform(s_df_label[\"student_country\"])\n",
    "s_df_label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLn8TU6ze5OY"
   },
   "source": [
    "# PART 3. **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_thre = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate predictors with a weak correlation with Critical Temperature (Y)\n",
    "corr_matrix = s_df_hot.corr().abs()\n",
    "to_drop = corr_matrix.loc[corr_matrix['purchased_yes_no'] < corr_thre]\n",
    "to_drop_names = []\n",
    "for row in to_drop.index:\n",
    "    to_drop_names.append(row)\n",
    "\n",
    "s_df_hot_0 = s_df_hot.drop(to_drop_names, axis=1)\n",
    "print(s_df_hot_0.shape)\n",
    "s_df_hot_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_hot_0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with Labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate predictors with a weak correlation with Critical Temperature (Y)\n",
    "corr_matrix = s_df_label.corr().abs()\n",
    "to_drop = corr_matrix.loc[corr_matrix['purchased_yes_no'] < corr_thre]\n",
    "to_drop_names = []\n",
    "for row in to_drop.index:\n",
    "    to_drop_names.append(row)\n",
    "\n",
    "s_df_label_0 = s_df_label.drop(to_drop_names, axis=1)\n",
    "print(s_df_label_0.shape)\n",
    "s_df_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df_label_0.columns\n",
    "\n",
    "s_df_label_0.columns == s_df_hot_0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4. **Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = s_df_hot_0.copy().astype(float).fillna(0)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting in training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to keep the random state I used my matricule Number\n",
    "seedNum = 931221\n",
    "val_size = 0.3\n",
    "\n",
    "# Let's divide the data sets in predictors and response variables\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "Y_set= new_df[['purchased_yes_no']]\n",
    "X_set_0 = new_df.drop(columns=['purchased_yes_no'])\n",
    "X_set = pd.DataFrame(scaler.fit_transform(X_set_0.values), columns=X_set_0.columns, index=X_set_0.index)\n",
    "\n",
    "# for red sets\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X_set, Y_set, test_size=val_size, random_state=seedNum)\n",
    "\n",
    "# Data set Predictors scal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5. **Model construction and Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since is a classification, the scroting for our models is Accuracy of the model.\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Classification Algorithms Array\n",
    "models = [\n",
    "    ['Linear DA', LinearDiscriminantAnalysis()],\n",
    "    ['Log Regression', LogisticRegression()],\n",
    "    ['KNN', KNeighborsClassifier()],\n",
    "    ['Class Tree', DecisionTreeClassifier(random_state = seedNum)],\n",
    "    ['SVM', SVC(random_state = seedNum)],\n",
    "    ['Naive Bayes', GaussianNB()],\n",
    "    ['Bagging', BaggingClassifier(random_state = seedNum)],\n",
    "    ['RF', RandomForestClassifier(random_state = seedNum)],\n",
    "    ['Extra Trees', ExtraTreesClassifier(random_state = seedNum)],\n",
    "    ['Ada-Booster', AdaBoostClassifier(random_state = seedNum)],\n",
    "    ['Grad-Booster', GradientBoostingClassifier(random_state = seedNum)],\n",
    "    ['XG Booster', xgb.XGBClassifier(random_state = seedNum) ]\n",
    "]\n",
    "# Run algorithms using n-fold cross validation\n",
    "folds = 10\n",
    "\n",
    "ModelsResults = []\n",
    "ModelsNames = []\n",
    "ModelsMeans = []\n",
    "models_dict = {}\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "for name, model in models:\n",
    "    startTimeModule = datetime.now()\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=seedNum)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    ModelsResults.append(cv_results)\n",
    "    ModelsNames.append(name)\n",
    "    ModelsMeans.append(cv_results.mean())\n",
    "    models_dict[name] = ([cv_results.mean(), cv_results.std()])\n",
    "    msg = \"%s:\\n  mean: %f  -  std: %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    print(\"Model training time:\", (datetime.now() - startTimeModule))\n",
    "# print('\\nAverage (' + scoring + ') from all models:', np.mean(ModelsMeans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning (Gradient Booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning algorithm #2 - Random Forest\n",
    "startTimeModule = datetime.now()\n",
    "# define models and parameters\n",
    "model = GradientBoostingClassifier(random_state=seedNum)\n",
    "# define grid search\n",
    "grid = {\n",
    "    \"n_estimators\": [5, 50, 250, 500],\n",
    "    \"max_depth\": [1, 3, 5, 7, 9],\n",
    "    \"learning_rate\": [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=folds, n_repeats=3, random_state=seedNum)\n",
    "grid2 = GridSearchCV(\n",
    "    estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring, error_score=0\n",
    ")\n",
    "grid_result2 = grid2.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\n",
    "means = grid_result2.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_result2.cv_results_[\"std_test_score\"]\n",
    "params = grid_result2.cv_results_[\"params\"]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Model training time:\", (datetime.now() - startTimeModule))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTU0ftotvVr8"
   },
   "source": [
    "# PART 6 - Select and Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-Booster --> Best: 0.971304 using {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
    "\n",
    "model = GradientBoostingClassifier(random_state = seedNum)\n",
    "model.fit(X_train, Y_train)\n",
    "model_Eval = model.predict(X_eval)\n",
    "\n",
    "\n",
    "print(accuracy_score(Y_eval, model_Eval))\n",
    "print(confusion_matrix(Y_eval, model_Eval))\n",
    "print(classification_report(Y_eval, model_Eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79c93df0d7ee6a3bd562a4aa4c384e2248e43cb5b1b6ee1056ccd166784507ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
